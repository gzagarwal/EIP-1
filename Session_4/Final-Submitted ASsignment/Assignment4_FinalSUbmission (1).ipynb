{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_FinalSUbmission.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvBc398HBFqv",
        "colab_type": "code",
        "outputId": "a85ea76a-f942-41b9-94f9-2010d9bbbd71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!pip install pydrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUEtFCgwBcaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import linecache\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from collections import Counter\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "tinyimagenet_base_url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "\n",
        "def reporthook(t):\n",
        "  \"\"\"https://github.com/tqdm/tqdm\"\"\"\n",
        "  last_b = [0]\n",
        "\n",
        "  def inner(b=1, bsize=1, tsize=None):\n",
        "    \"\"\"\n",
        "    b: int, optional\n",
        "        Number of blocks just transferred [default: 1].\n",
        "    bsize: int, optional\n",
        "        Size of each block (in tqdm units) [default: 1].\n",
        "    tsize: int, optional\n",
        "        Total size (in tqdm units). If [default: None] remains unchanged.\n",
        "    \"\"\"\n",
        "    if tsize is not None:\n",
        "        t.total = tsize\n",
        "    t.update((b - last_b[0]) * bsize)\n",
        "    last_b[0] = b\n",
        "  return inner\n",
        "\n",
        "def maybe_download(url, filename, prefix, num_bytes=None):\n",
        "    \"\"\"Takes an URL, a filename, and the expected bytes, download\n",
        "    the contents and returns the filename\n",
        "    num_bytes=None disables the file size check.\"\"\"\n",
        "    local_filename = None\n",
        "    if not os.path.exists(os.path.join(prefix, filename)):\n",
        "        try:\n",
        "            print(\"Downloading file {}...\".format(url + filename))\n",
        "            with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
        "                local_filename, _ = urlretrieve(url + filename, os.path.join(prefix,filename), reporthook=reporthook(t))\n",
        "        except AttributeError as e:\n",
        "            print(\"An error occurred when downloading the file! Please get the dataset using a browser.\")\n",
        "            raise e\n",
        "    # We have a downloaded file\n",
        "    # Check the stats and make sure they are ok\n",
        "    file_stats = os.stat(os.path.join(prefix,filename))\n",
        "    if num_bytes is None or file_stats.st_size == num_bytes:\n",
        "        print(\"File {} successfully loaded\".format(filename))\n",
        "    else:\n",
        "        raise Exception(\"Unexpected dataset size. Please get the dataset using a browser.\")\n",
        "\n",
        "    return local_filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5liG0440Bgfq",
        "colab_type": "code",
        "outputId": "6325689b-bd13-4574-97e9-75831531072f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tinyimagenet_base_url = \"http://cs231n.stanford.edu/\"\n",
        "    tinyimagenet_filename = \"tiny-imagenet-200.zip\"\n",
        "    unzipped_filename = \"tiny-imagenet-200\"\n",
        "\n",
        "    prefix = os.path.join(\"download\", \"dwr\")\n",
        "    data_prefix = os.path.join(\"data\")\n",
        "\n",
        "    print(\"Storing datasets in {}\".format(prefix))\n",
        "\n",
        "    if not os.path.exists(prefix):\n",
        "        os.makedirs(prefix)\n",
        "    if not os.path.exists(data_prefix):\n",
        "        os.makedirs(data_prefix)\n",
        "    \n",
        "    glove_zip = maybe_download(tinyimagenet_base_url, tinyimagenet_filename, prefix, None)\n",
        "\n",
        "    print(\"Extracting\")\n",
        "    glove_zip_ref = zipfile.ZipFile(os.path.join(prefix, tinyimagenet_filename), 'r')\n",
        "\n",
        "    glove_zip_ref.extractall(prefix)\n",
        "    glove_zip_ref.close()\n",
        "    \n",
        "    print(\"Moving datasets to {}\".format(data_prefix))\n",
        "    shutil.move(os.path.join(prefix, unzipped_filename), os.path.join(data_prefix, unzipped_filename))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Storing datasets in download/dwr\n",
            "File tiny-imagenet-200.zip successfully loaded\n",
            "Extracting\n",
            "Moving datasets to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CQh8iw8BTli",
        "colab_type": "code",
        "outputId": "a03f05b1-48aa-4538-b8f2-7213bde47d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# ###################################################### This is the Block WIth 32x32 and no AUgmentation Images  ####################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "BATCH_SIZE=500\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    #zoom_range = 0.2,\n",
        "    #rotation_range=40,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True,\n",
        "    #shear_range=0.2,\n",
        "    #fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory( r'./data/tiny-imagenet-200/train/', target_size=(32,32),  \n",
        "                                                    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True)\n",
        "val_data = pd.read_csv(r'./data/tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "VAL_BATCH_SIZE=500\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory=r'./data/tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                     class_mode='categorical', batch_size=VAL_BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwF7nTjgxFEx",
        "colab_type": "code",
        "outputId": "49aec9af-2b6e-4756-f378-da23c08a3c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1908
        }
      },
      "source": [
        "from keras import backend as tf\n",
        "\n",
        "###################################################### This is the Block WIth Custom Architecure, No ResNet ,But Rreplacement of Dense Layer withGAP for 200 CLasses    ####################################\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=4)\n",
        "\n",
        "# This can be done to support Multiple Shape Types as Input\n",
        "input = Input(shape=(None,None,3,))\n",
        "\n",
        "#def space_to_depth_x2(x):\n",
        "#    return tf.space_to_depth(x, block_size=8)\n",
        "#input = Input(shape=(64,64,3))\n",
        "\n",
        "from keras.layers import SeparableConv2D,DepthwiseConv2D,Convolution2D,GlobalAveragePooling2D\n",
        "#Block1\n",
        "layer1 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2_block_32', use_bias=False)(input)\n",
        "layer1 = BatchNormalization(name='norm_1_block_1')(layer1)\n",
        "layer1 = ReLU()(layer1)\n",
        "\n",
        "# Layer 2\n",
        "layer2 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_2_block_1', use_bias=False)(layer1)\n",
        "layer2 = BatchNormalization(name='norm_1_block_layer2')(layer2)\n",
        "layer2 = ReLU()(layer2)\n",
        "\n",
        "\n",
        "\n",
        "layer4 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_4_block_1', use_bias=False)(layer2)\n",
        "layer4 = BatchNormalization(name='norm_1_4_block_1')(layer4)\n",
        "layer4 = ReLU()(layer4)\n",
        "\n",
        "layer5 = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4_16', use_bias=False)(layer4)\n",
        "layer5 = BatchNormalization(name='norm_4_16')(layer5)\n",
        "layer5 = ReLU()(layer5)\n",
        "\n",
        "layer5_Max = MaxPooling2D(pool_size=(2, 2))(layer5)\n",
        "\n",
        "skip_connection = layer5_Max\n",
        "\n",
        "\n",
        "# Layer 3\n",
        "layer6 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3_block_2', use_bias=False)(layer5_Max)\n",
        "layer6 = BatchNormalization(name='norm_1_3_block_2')(layer6)\n",
        "layer6 = ReLU()(layer6)\n",
        "\n",
        "#layer4\n",
        "\n",
        "layer6_1 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_3_block_2_1', use_bias=False)(layer6)\n",
        "layer6_1 = BatchNormalization(name='norm_1_3_block_2_1')(layer6_1)\n",
        "layer6_1 = ReLU()(layer6_1)\n",
        "\n",
        "layer6_2 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_3_block_2_2', use_bias=False)(layer6_1)\n",
        "layer6_2 = BatchNormalization(name='norm_1_3_block_2_2')(layer6_2)\n",
        "layer6_2 = ReLU()(layer6_2)\n",
        "\n",
        "#layer4\n",
        "\n",
        "layer14_64_21 = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7_block_3_1_layer14_62', use_bias=False)(layer6_2)\n",
        "layer14_64_21 = BatchNormalization(name='conv_7_block_3_1_layer14_611')(layer14_64_21)\n",
        "layer14_64_21 = ReLU()(layer14_64_21)\n",
        "\n",
        "\n",
        "layer14_Max_64 = MaxPooling2D(pool_size=(2, 2))(layer14_64_21)\n",
        "\n",
        "\n",
        "\n",
        "layer14_64_2 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_7_block__1', use_bias=False)(layer14_Max_64)\n",
        "layer14_64_2 = BatchNormalization(name='norm_14_layr4_64')(layer14_64_2)\n",
        "layer14_64_2 = ReLU()(layer14_64_2)\n",
        "\n",
        "\n",
        "layer14_2 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_7_block__1a', use_bias=False)(layer14_64_2)\n",
        "layer14_2 = BatchNormalization(name='norm_14_layr4_64a')(layer14_2)\n",
        "layer14_2 = ReLU()(layer14_2)\n",
        "\n",
        "\n",
        "layer14_64_256 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='layer14_64_256A', use_bias=False)(layer14_2)\n",
        "layer14_64_256 = BatchNormalization(name='layer14_64_256')(layer14_64_256)\n",
        "layer14_64_256 = ReLU()(layer14_64_256)\n",
        "\n",
        "layer14_Max_256 = MaxPooling2D(pool_size=(2, 2))(layer14_64_256)\n",
        "\n",
        "\n",
        "# Layer 15\n",
        "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='skipLayer', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='skip_connection')(skip_connection)\n",
        "skip_connection = ReLU()(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "layer15 = concatenate([skip_connection, layer14_Max_256])\n",
        "\n",
        "\n",
        "\n",
        "layer19_1_Max_O_2_K = MaxPooling2D(pool_size=(2,2))(layer15)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "layer20 = Convolution2D(200,(1,1),strides=(1,1),padding='same', name='conv_1EP', use_bias=False)(layer19_1_Max_O_2_K)\n",
        "# This is the replacement of Dense Layer\n",
        "layer21=GlobalAveragePooling2D(data_format='channels_last')(layer20)\n",
        "\n",
        "output= Activation('softmax')(layer21)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_2_block_32 (Conv2D)        (None, None, None, 6 1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1_block_1 (BatchNormalizat (None, None, None, 6 256         conv_2_block_32[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, None, None, 6 0           norm_1_block_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_2_block_1 (Conv2D)         (None, None, None, 1 73728       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1_block_layer2 (BatchNorma (None, None, None, 1 512         conv_2_block_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, None, None, 1 0           norm_1_block_layer2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv_4_block_1 (Conv2D)         (None, None, None, 2 294912      re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1_4_block_1 (BatchNormaliz (None, None, None, 2 1024        conv_4_block_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, None, None, 2 0           norm_1_4_block_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_4_16 (Conv2D)              (None, None, None, 6 16384       re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_4_16 (BatchNormalization)  (None, None, None, 6 256         conv_4_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, None, None, 6 0           norm_4_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3_block_2 (Conv2D)         (None, None, None, 1 73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_1_3_block_2 (BatchNormaliz (None, None, None, 1 512         conv_3_block_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, None, None, 1 0           norm_1_3_block_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_3_block_2_1 (Conv2D)       (None, None, None, 2 294912      re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1_3_block_2_1 (BatchNormal (None, None, None, 2 1024        conv_3_block_2_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, None, None, 2 0           norm_1_3_block_2_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv_3_block_2_2 (Conv2D)       (None, None, None, 5 1179648     re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1_3_block_2_2 (BatchNormal (None, None, None, 5 2048        conv_3_block_2_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, None, None, 5 0           norm_1_3_block_2_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv_7_block_3_1_layer14_62 (Co (None, None, None, 1 65536       re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_7_block_3_1_layer14_611 (B (None, None, None, 1 512         conv_7_block_3_1_layer14_62[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, None, None, 1 0           conv_7_block_3_1_layer14_611[0][0\n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_7_block__1 (Conv2D)        (None, None, None, 2 294912      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_14_layr4_64 (BatchNormaliz (None, None, None, 2 1024        conv_7_block__1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, None, None, 2 0           norm_14_layr4_64[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_7_block__1a (Conv2D)       (None, None, None, 5 1179648     re_lu_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_14_layr4_64a (BatchNormali (None, None, None, 5 2048        conv_7_block__1a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, None, None, 5 0           norm_14_layr4_64a[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skipLayer (Conv2D)              (None, None, None, 6 4096        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer14_64_256A (Conv2D)        (None, None, None, 1 4718592     re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "skip_connection (BatchNormaliza (None, None, None, 6 256         skipLayer[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer14_64_256 (BatchNormalizat (None, None, None, 1 4096        layer14_64_256A[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, None, None, 6 0           skip_connection[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, None, None, 1 0           layer14_64_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, None, None, 1 0           re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 1 0           re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 2 0           lambda_1[0][0]                   \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 2 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1EP (Conv2D)               (None, None, None, 2 409600      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 200)          0           conv_1EP[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 200)          0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 8,620,992\n",
            "Trainable params: 8,614,208\n",
            "Non-trainable params: 6,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyKm_YuHCV86",
        "colab_type": "code",
        "outputId": "51a90f95-4d67-4aad-d47e-3f1a129fe431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        }
      },
      "source": [
        "#Model Fit Generator\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "reduce = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "\n",
        "filepath = \"/content/gdrive/My Drive/model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
        "\n",
        "callbacks=[reduce,checkpoint]\n",
        "NUM_EPOCHS=10\n",
        "\n",
        "# make sure to update the steps_per_epoch & validation_steps properly\n",
        "# in case you are using custom generator with aug for validation data as well, then replace VAL_IMAGES with validation_batches.sample\n",
        "# NOTE: do not forget to use // for dividing, as it returns an int. just / returns float\n",
        "# NOTE: int() in caculating steps_per_epoch & validation_steps is not required as dividing by // returns int\n",
        "history = model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(train_generator.n//BATCH_SIZE), \n",
        "                    epochs=NUM_EPOCHS, verbose=1,\n",
        "                    validation_steps=int(validation_generator.n//BATCH_SIZE), \n",
        "                    validation_data=validation_generator,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 286s 1s/step - loss: 4.7637 - acc: 0.0643 - val_loss: 5.8891 - val_acc: 0.0238\n",
            "\n",
            "Epoch 00001: loss improved from inf to 4.76370, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 255s 1s/step - loss: 4.0063 - acc: 0.1478 - val_loss: 5.0056 - val_acc: 0.0586\n",
            "\n",
            "Epoch 00002: loss improved from 4.76370 to 4.00629, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 255s 1s/step - loss: 3.6702 - acc: 0.1986 - val_loss: 4.5975 - val_acc: 0.0764\n",
            "\n",
            "Epoch 00003: loss improved from 4.00629 to 3.67017, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 255s 1s/step - loss: 3.4066 - acc: 0.2412 - val_loss: 4.2860 - val_acc: 0.1161\n",
            "\n",
            "Epoch 00004: loss improved from 3.67017 to 3.40663, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 253s 1s/step - loss: 3.2135 - acc: 0.2754 - val_loss: 4.3885 - val_acc: 0.1011\n",
            "\n",
            "Epoch 00005: loss improved from 3.40663 to 3.21351, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 253s 1s/step - loss: 3.0229 - acc: 0.3089 - val_loss: 4.0808 - val_acc: 0.1418\n",
            "\n",
            "Epoch 00006: loss improved from 3.21351 to 3.02288, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 254s 1s/step - loss: 2.8674 - acc: 0.3403 - val_loss: 3.9998 - val_acc: 0.1616\n",
            "\n",
            "Epoch 00007: loss improved from 3.02288 to 2.86745, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 254s 1s/step - loss: 2.6820 - acc: 0.3714 - val_loss: 4.1471 - val_acc: 0.1396\n",
            "\n",
            "Epoch 00008: loss improved from 2.86745 to 2.68203, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 254s 1s/step - loss: 2.5467 - acc: 0.3997 - val_loss: 3.7476 - val_acc: 0.1759\n",
            "\n",
            "Epoch 00009: loss improved from 2.68203 to 2.54671, saving model to /content/gdrive/My Drive/model.h5\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 253s 1s/step - loss: 2.3932 - acc: 0.4289 - val_loss: 4.0430 - val_acc: 0.1516\n",
            "\n",
            "Epoch 00010: loss improved from 2.54671 to 2.39320, saving model to /content/gdrive/My Drive/model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnNfcsKQEMYQ",
        "colab_type": "code",
        "outputId": "3298b90e-8d07-439e-b27c-531d96fb3d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# This is the Block which will train for 16x16 images and validation of 64x64 with no Augmnetation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "BATCH_SIZE=400\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    #zoom_range = 0.2,\n",
        "    #rotation_range=40,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True,\n",
        "    #shear_range=0.2,\n",
        "    #fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory( r'./data/tiny-imagenet-200/train/', target_size=(16,16),  \n",
        "                                                    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True)\n",
        "val_data = pd.read_csv(r'./data/tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory=r'./data/tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhA2bGloFF3p",
        "colab_type": "code",
        "outputId": "4bf7a526-1bf8-4833-f09c-59e966c859d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        }
      },
      "source": [
        "# It will run for the above 16x16 block with 10 epochs and no augmnetation\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.backend import tf\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "# use the generators as usual in fit_generator\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "new_model = load_model(\"/content/gdrive/My Drive/model.h5\", custom_objects={'tf': tf})\n",
        "NUM_EPOCHS=10\n",
        "filepath=\"model_16_10From_model.h5\"\n",
        "# fit the model\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "clr_triangular = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "callbacks = [checkpoint,clr_triangular]\n",
        "new_model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(train_generator.n//BATCH_SIZE), \n",
        "                    epochs=NUM_EPOCHS, verbose=1,\n",
        "                    validation_steps=int(validation_generator.n//BATCH_SIZE), \n",
        "  \n",
        "                    validation_data=validation_generator,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 153s 612ms/step - loss: 3.3976 - acc: 0.2503 - val_loss: 5.9448 - val_acc: 0.0525\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.39761, saving model to model_16_10From_model.h5\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 133s 531ms/step - loss: 3.0497 - acc: 0.3102 - val_loss: 5.8402 - val_acc: 0.0468\n",
            "\n",
            "Epoch 00002: loss improved from 3.39761 to 3.04969, saving model to model_16_10From_model.h5\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 132s 527ms/step - loss: 2.8092 - acc: 0.3509 - val_loss: 6.3266 - val_acc: 0.0394\n",
            "\n",
            "Epoch 00003: loss improved from 3.04969 to 2.80916, saving model to model_16_10From_model.h5\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 132s 528ms/step - loss: 2.5405 - acc: 0.4004 - val_loss: 6.0858 - val_acc: 0.0469\n",
            "\n",
            "Epoch 00004: loss improved from 2.80916 to 2.54046, saving model to model_16_10From_model.h5\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 132s 527ms/step - loss: 2.3222 - acc: 0.4444 - val_loss: 6.4373 - val_acc: 0.0398\n",
            "\n",
            "Epoch 00005: loss improved from 2.54046 to 2.32217, saving model to model_16_10From_model.h5\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 132s 529ms/step - loss: 2.2146 - acc: 0.4708 - val_loss: 9.3078 - val_acc: 0.0249\n",
            "\n",
            "Epoch 00006: loss improved from 2.32217 to 2.21456, saving model to model_16_10From_model.h5\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 132s 527ms/step - loss: 1.9659 - acc: 0.5180 - val_loss: 5.9480 - val_acc: 0.0496\n",
            "\n",
            "Epoch 00007: loss improved from 2.21456 to 1.96586, saving model to model_16_10From_model.h5\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 131s 524ms/step - loss: 1.2993 - acc: 0.6778 - val_loss: 6.4520 - val_acc: 0.0492\n",
            "\n",
            "Epoch 00008: loss improved from 1.96586 to 1.29934, saving model to model_16_10From_model.h5\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 132s 529ms/step - loss: 1.0163 - acc: 0.7375 - val_loss: 7.0990 - val_acc: 0.0394\n",
            "\n",
            "Epoch 00009: loss improved from 1.29934 to 1.01632, saving model to model_16_10From_model.h5\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 132s 528ms/step - loss: 0.7555 - acc: 0.8115 - val_loss: 7.5941 - val_acc: 0.0344\n",
            "\n",
            "Epoch 00010: loss improved from 1.01632 to 0.75553, saving model to model_16_10From_model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc37ce0dcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TmWtCejDfbv",
        "colab_type": "code",
        "outputId": "72ba920c-bbf3-4dd2-94c0-091fb115e748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "###################################################### This is the  Block, WIth 16x16 Images and zoom_range of 0.2 with BATCH SIZE of 400 ####################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "BATCH_SIZE=400\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    zoom_range = 0.2,\n",
        "    #rotation_range=40,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True,\n",
        "    #shear_range=0.2,\n",
        "    #fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory( r'./data/tiny-imagenet-200/train/', target_size=(16,16),  \n",
        "                                                    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True)\n",
        "val_data = pd.read_csv(r'./data/tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory=r'./data/tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, shuffle=True)\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.backend import tf\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "# use the generators as usual in fit_generator\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "new_model = load_model(\"model_16_10From_model.h5\", custom_objects={'tf': tf})\n",
        "NUM_EPOCHS=10\n",
        "filepath=\"/content/gdrive/My Drive/model_16_10From_model_zoom.h5\"\n",
        "# fit the model\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "clr_triangular = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "callbacks = [checkpoint,clr_triangular]\n",
        "new_model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(train_generator.n//BATCH_SIZE), \n",
        "                    epochs=NUM_EPOCHS, verbose=1,\n",
        "                    validation_steps=int(validation_generator.n//BATCH_SIZE), \n",
        "  \n",
        "                    validation_data=validation_generator,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 168s 671ms/step - loss: 1.7960 - acc: 0.5326 - val_loss: 6.9510 - val_acc: 0.0436\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.79596, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 159s 638ms/step - loss: 1.5934 - acc: 0.5799 - val_loss: 6.8476 - val_acc: 0.0484\n",
            "\n",
            "Epoch 00002: loss improved from 1.79596 to 1.59341, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 159s 634ms/step - loss: 1.4478 - acc: 0.6169 - val_loss: 7.1089 - val_acc: 0.0441\n",
            "\n",
            "Epoch 00003: loss improved from 1.59341 to 1.44777, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 158s 630ms/step - loss: 1.3356 - acc: 0.6432 - val_loss: 6.9263 - val_acc: 0.0414\n",
            "\n",
            "Epoch 00004: loss improved from 1.44777 to 1.33556, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 159s 636ms/step - loss: 1.2348 - acc: 0.6701 - val_loss: 7.0732 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00005: loss improved from 1.33556 to 1.23483, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 158s 633ms/step - loss: 1.1336 - acc: 0.6980 - val_loss: 7.1145 - val_acc: 0.0439\n",
            "\n",
            "Epoch 00006: loss improved from 1.23483 to 1.13363, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 159s 634ms/step - loss: 1.0461 - acc: 0.7198 - val_loss: 7.2104 - val_acc: 0.0396\n",
            "\n",
            "Epoch 00007: loss improved from 1.13363 to 1.04614, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 158s 632ms/step - loss: 0.8416 - acc: 0.7852 - val_loss: 7.1413 - val_acc: 0.0409\n",
            "\n",
            "Epoch 00008: loss improved from 1.04614 to 0.84161, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 158s 633ms/step - loss: 0.7924 - acc: 0.8005 - val_loss: 7.1135 - val_acc: 0.0432\n",
            "\n",
            "Epoch 00009: loss improved from 0.84161 to 0.79236, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 158s 631ms/step - loss: 0.7574 - acc: 0.8109 - val_loss: 7.2409 - val_acc: 0.0405\n",
            "\n",
            "Epoch 00010: loss improved from 0.79236 to 0.75740, saving model to /content/gdrive/My Drive/model_16_10From_model_zoom.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc37b7bb048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwwlsougKbM6",
        "colab_type": "code",
        "outputId": "71ccb1e2-b7dc-4249-82c2-9b8c38dcfdd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "###################################################### This is the  Block, WIth 56x56 Images and BATCH SIZE=250 ####################################\n",
        "                                                    #Batch SIz reduced Otherwise gets OOME with Increased Image SIze.\n",
        "# Use Augmentaion parameters as required.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "BATCH_SIZE=250\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    #zoom_range = 0.2,\n",
        "    #rotation_range=40,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True,\n",
        "    #shear_range=0.2,\n",
        "    #fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory( r'./data/tiny-imagenet-200/train/', target_size=(56,56),  \n",
        "                                                    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True)\n",
        "val_data = pd.read_csv(r'./data/tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory=r'./data/tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, shuffle=True)\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.backend import tf\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "# use the generators as usual in fit_generator\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "new_model = load_model(\"/content/gdrive/My Drive/model_16_10From_model_zoom.h5\", custom_objects={'tf': tf})\n",
        "NUM_EPOCHS=10\n",
        "filepath=\"/content/gdrive/My Drive/model_56.h5\"\n",
        "# fit the model\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "clr_triangular = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "callbacks = [checkpoint,clr_triangular]\n",
        "new_model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(train_generator.n//BATCH_SIZE), \n",
        "                    epochs=NUM_EPOCHS, verbose=1,\n",
        "                    validation_steps=int(validation_generator.n//BATCH_SIZE), \n",
        "  \n",
        "                    validation_data=validation_generator,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "Epoch 1/10\n",
            "400/400 [==============================] - 756s 2s/step - loss: 2.9541 - acc: 0.3111 - val_loss: 3.0415 - val_acc: 0.2968\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.95408, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 2/10\n",
            "400/400 [==============================] - 732s 2s/step - loss: 2.5067 - acc: 0.3975 - val_loss: 2.8605 - val_acc: 0.3285\n",
            "\n",
            "Epoch 00002: loss improved from 2.95408 to 2.50674, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 3/10\n",
            "400/400 [==============================] - 731s 2s/step - loss: 2.3094 - acc: 0.4388 - val_loss: 2.7357 - val_acc: 0.3549\n",
            "\n",
            "Epoch 00003: loss improved from 2.50674 to 2.30941, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 4/10\n",
            "400/400 [==============================] - 733s 2s/step - loss: 2.1676 - acc: 0.4705 - val_loss: 2.6556 - val_acc: 0.3726\n",
            "\n",
            "Epoch 00004: loss improved from 2.30941 to 2.16765, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 5/10\n",
            "400/400 [==============================] - 726s 2s/step - loss: 2.0475 - acc: 0.4970 - val_loss: 2.6034 - val_acc: 0.3838\n",
            "\n",
            "Epoch 00005: loss improved from 2.16765 to 2.04748, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 6/10\n",
            "400/400 [==============================] - 728s 2s/step - loss: 1.9450 - acc: 0.5185 - val_loss: 2.5704 - val_acc: 0.3893\n",
            "\n",
            "Epoch 00006: loss improved from 2.04748 to 1.94498, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 7/10\n",
            "400/400 [==============================] - 727s 2s/step - loss: 1.8506 - acc: 0.5424 - val_loss: 2.5259 - val_acc: 0.4019\n",
            "\n",
            "Epoch 00007: loss improved from 1.94498 to 1.85061, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 8/10\n",
            "400/400 [==============================] - 726s 2s/step - loss: 1.7623 - acc: 0.5623 - val_loss: 2.5228 - val_acc: 0.3993\n",
            "\n",
            "Epoch 00008: loss improved from 1.85061 to 1.76231, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 9/10\n",
            "400/400 [==============================] - 729s 2s/step - loss: 1.6771 - acc: 0.5835 - val_loss: 2.4648 - val_acc: 0.4136\n",
            "\n",
            "Epoch 00009: loss improved from 1.76231 to 1.67709, saving model to /content/gdrive/My Drive/model_56.h5\n",
            "Epoch 10/10\n",
            "400/400 [==============================] - 731s 2s/step - loss: 1.5948 - acc: 0.6045 - val_loss: 2.4727 - val_acc: 0.4120\n",
            "\n",
            "Epoch 00010: loss improved from 1.67709 to 1.59479, saving model to /content/gdrive/My Drive/model_56.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc375902518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YACDX_KRnksk",
        "colab_type": "code",
        "outputId": "5071ba71-671d-463d-c92a-46f72db3010b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1543
        }
      },
      "source": [
        "###################################################### This is the  Block, WIth 64x64 Images Only and Validat_accuract of 49 with accu: 90(OVERFITTING) and BATCH SIZE 200 #######\n",
        "# Use Augmentaion parameters as required.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "BATCH_SIZE=200\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    #zoom_range = 0.2,\n",
        "    #rotation_range=40,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True,\n",
        "    #shear_range=0.2,\n",
        "    #fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory( r'./data/tiny-imagenet-200/train/', target_size=(64,64),  \n",
        "                                                    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True)\n",
        "val_data = pd.read_csv(r'./data/tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory=r'./data/tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                     class_mode='categorical', batch_size=150, shuffle=True)\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.backend import tf\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "# use the generators as usual in fit_generator\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "new_model = load_model(\"/content/gdrive/My Drive/model_56.h5\", custom_objects={'tf': tf})\n",
        "print(new_model)\n",
        "NUM_EPOCHS=20\n",
        "filepath=\"/content/gdrive/My Drive/model_64.h5\"\n",
        "# fit the model\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "clr_triangular = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=12, min_lr=0.5e-6)\n",
        "callbacks = [checkpoint,clr_triangular]\n",
        "new_model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(train_generator.n//BATCH_SIZE), \n",
        "                    epochs=NUM_EPOCHS, verbose=1,\n",
        "                    validation_steps=int(validation_generator.n//BATCH_SIZE), \n",
        "  \n",
        "                    validation_data=validation_generator,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "<keras.engine.training.Model object at 0x7f40d6e6f780>\n",
            "Epoch 1/20\n",
            "500/500 [==============================] - 939s 2s/step - loss: 1.9322 - acc: 0.5250 - val_loss: 2.3422 - val_acc: 0.4431\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.44307, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 939s 2s/step - loss: 1.7973 - acc: 0.5541 - val_loss: 2.3329 - val_acc: 0.4400\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.44307\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 1.6977 - acc: 0.5764 - val_loss: 2.2862 - val_acc: 0.4499\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.44307 to 0.44993, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 1.6086 - acc: 0.5991 - val_loss: 2.2592 - val_acc: 0.4625\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.44993 to 0.46253, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 1.5274 - acc: 0.6198 - val_loss: 2.2241 - val_acc: 0.4711\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.46253 to 0.47114, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 1.4499 - acc: 0.6387 - val_loss: 2.2514 - val_acc: 0.4648\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.47114\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 1.3779 - acc: 0.6578 - val_loss: 2.1622 - val_acc: 0.4812\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.47114 to 0.48121, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 1.3068 - acc: 0.6766 - val_loss: 2.1790 - val_acc: 0.4833\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.48121 to 0.48333, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 1.2400 - acc: 0.6934 - val_loss: 2.2053 - val_acc: 0.4792\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.48333\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 1.1759 - acc: 0.7138 - val_loss: 2.2070 - val_acc: 0.4752\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.48333\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 1.1106 - acc: 0.7312 - val_loss: 2.2096 - val_acc: 0.4752\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.48333\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 1.0484 - acc: 0.7480 - val_loss: 2.1638 - val_acc: 0.4891\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.48333 to 0.48907, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 0.9844 - acc: 0.7672 - val_loss: 2.2368 - val_acc: 0.4722\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.48907\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 0.9282 - acc: 0.7837 - val_loss: 2.2037 - val_acc: 0.4805\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.48907\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 934s 2s/step - loss: 0.8676 - acc: 0.8008 - val_loss: 2.1654 - val_acc: 0.4925\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.48907 to 0.49248, saving model to /content/gdrive/My Drive/model_64.h5\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 933s 2s/step - loss: 0.8106 - acc: 0.8185 - val_loss: 2.2183 - val_acc: 0.4736\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.49248\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 936s 2s/step - loss: 0.7539 - acc: 0.8337 - val_loss: 2.2275 - val_acc: 0.4863\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.49248\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 936s 2s/step - loss: 0.7013 - acc: 0.8504 - val_loss: 2.2140 - val_acc: 0.4842\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.49248\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 0.6479 - acc: 0.8661 - val_loss: 2.2860 - val_acc: 0.4817\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.49248\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 935s 2s/step - loss: 0.5314 - acc: 0.9094 - val_loss: 2.1833 - val_acc: 0.4933\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.49248 to 0.49333, saving model to /content/gdrive/My Drive/model_64.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f40d5c6a2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXT7lWUQ6-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CYCLIC LEARNIN RATE\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency.\n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored\n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or\n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "\n",
        "    # Example for CIFAR-10 w/ batch size 100:\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    # References\n",
        "\n",
        "      - [Cyclical Learning Rates for Training Neural Networks](\n",
        "      https://arxiv.org/abs/1506.01186)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            base_lr=0.001,\n",
        "            max_lr=0.006,\n",
        "            step_size=2000.,\n",
        "            mode='triangular',\n",
        "            gamma=1.,\n",
        "            scale_fn=None,\n",
        "            scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2',\n",
        "                        'exp_range']:\n",
        "            raise KeyError(\"mode must be one of 'triangular', \"\n",
        "                           \"'triangular2', or 'exp_range'\")\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn is None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** x\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr is not None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr is not None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size is not None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "        self.history.setdefault(\n",
        "            'lr', []).append(\n",
        "            K.get_value(\n",
        "                self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZtGMY_fN7f",
        "colab_type": "code",
        "outputId": "2ca196a7-3e33-4871-d660-f0ef4dd74392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3007
        }
      },
      "source": [
        "###################################################### This is the final Block, WIth 64x64 Images, CYCLIC LEARNING RATE and IMAGE AUGMENTATION ####################################\n",
        "# Use Augmentaion parameters as required.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.backend import tf\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "# use the generators as usual in fit_generator\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "seq = iaa.Sequential([iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
        "        iaa.CropAndPad(\n",
        "            #px=((0, 30), (0, 10), (0, 30), (0, 10)),\n",
        "            percent=(-0.05, 0.1),\n",
        "            pad_mode=ia.ALL,\n",
        "            pad_cval=(0, 256)\n",
        "            ),\n",
        "        \n",
        "       \n",
        "        iaa.Flipud(0.2), # vertically flip 20% of all images\n",
        "\n",
        "        # crop some of the images by 0-10% of their height/width\n",
        "        sometimes(iaa.Crop(percent=(0, 0.5))),\n",
        "                      # Sharpen each image, overlay the result with the original\n",
        "                # image using an alpha between 0 (no sharpening) and 1\n",
        "                # (full sharpening effect).\n",
        "        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.5, 1.5)),\n",
        "\n",
        "                # Same as sharpen, but for an embossing effect.\n",
        "        iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "        iaa.CoarseDropout((0.0,0.03),size_percent=(0.01,0.15)),\n",
        "        iaa.Multiply((0.5, 1.5), per_channel=0.5)])\n",
        "\n",
        "\n",
        "#BATCH SIZE\n",
        "BATCH_SIZE=200\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    #zoom_range = 0.2,\n",
        "    preprocessing_function=seq.augment_image\n",
        "    #fill_mode='nearest'\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True,\n",
        "    #shear_range=0.2,\n",
        "    \n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory( r'./data/tiny-imagenet-200/train/', target_size=(64,64),  \n",
        "                                                    batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True,\n",
        "                                                    color_mode='rgb',interpolation='bilinear')\n",
        "\n",
        "\n",
        "print(train_generator)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val_data = pd.read_csv(r'./data/tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory=r'./data/tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                     class_mode='categorical', batch_size=150, shuffle=True)\n",
        "\n",
        "new_model_64 = load_model(\"/content/gdrive/My Drive/model_64.h5\", custom_objects={'tf': tf})\n",
        "print(new_model_64)\n",
        "NUM_EPOCHS=50\n",
        "filepath=\"/content/gdrive/My Drive/model_64_learn.h5\"\n",
        "# fit the model\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "#CYclic Learning Rate and mode is \"Triangular2\" HERE STEP_SIZE is increased , THIS PLAYS MAJOR ROLE IN INCREASING THE ACCURACY.\n",
        "clr = CyclicLR(base_lr=0.00001, max_lr=0.008,step_size=5000, mode='triangular2')\n",
        "\n",
        "# CallBack For CHeck POint AND Cyclic Learning Rate\n",
        "callbacks = [checkpoint,clr]\n",
        "\n",
        "#Load the Fit_generator with previous Trained model that learnt only on 64x64 images.\n",
        "new_model_64.fit_generator(train_generator,\n",
        "                    steps_per_epoch=int(train_generator.n//BATCH_SIZE), \n",
        "                    epochs=NUM_EPOCHS, verbose=1,\n",
        "                    validation_steps=int(validation_generator.n//150), \n",
        "  \n",
        "                    validation_data=validation_generator,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "<keras_preprocessing.image.directory_iterator.DirectoryIterator object at 0x7f18b021b7b8>\n",
            "Found 10000 images belonging to 200 classes.\n",
            "<keras.engine.training.Model object at 0x7f18b3ca4668>\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 876s 2s/step - loss: 4.4840 - acc: 0.1202 - val_loss: 3.9100 - val_acc: 0.1589\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.15889, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 4.0068 - acc: 0.1604 - val_loss: 3.4676 - val_acc: 0.2189\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.15889 to 0.21888, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 3.8108 - acc: 0.1806 - val_loss: 3.8406 - val_acc: 0.1698\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.21888\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 3.7034 - acc: 0.1974 - val_loss: 3.5462 - val_acc: 0.2159\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.21888\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 3.6623 - acc: 0.2048 - val_loss: 3.4563 - val_acc: 0.2288\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.21888 to 0.22883, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 3.5666 - acc: 0.2218 - val_loss: 3.5255 - val_acc: 0.2159\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.22883\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 3.4898 - acc: 0.2319 - val_loss: 3.9486 - val_acc: 0.2053\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.22883\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 3.4182 - acc: 0.2465 - val_loss: 3.4324 - val_acc: 0.2369\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.22883 to 0.23685, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 3.3639 - acc: 0.2561 - val_loss: 3.6020 - val_acc: 0.2254\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.23685\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 3.3089 - acc: 0.2642 - val_loss: 3.7070 - val_acc: 0.2042\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.23685\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 3.2386 - acc: 0.2784 - val_loss: 2.9653 - val_acc: 0.3201\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.23685 to 0.32010, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 869s 2s/step - loss: 3.1178 - acc: 0.3007 - val_loss: 2.9641 - val_acc: 0.3327\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.32010 to 0.33269, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 3.0072 - acc: 0.3208 - val_loss: 2.6774 - val_acc: 0.3814\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.33269 to 0.38142, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 2.9042 - acc: 0.3398 - val_loss: 2.6583 - val_acc: 0.3840\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.38142 to 0.38396, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.8143 - acc: 0.3574 - val_loss: 2.7203 - val_acc: 0.3772\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.38396\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.7226 - acc: 0.3770 - val_loss: 2.3990 - val_acc: 0.4421\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.38396 to 0.44213, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 873s 2s/step - loss: 2.6120 - acc: 0.4004 - val_loss: 2.6232 - val_acc: 0.4028\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.44213\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.5025 - acc: 0.4188 - val_loss: 2.0471 - val_acc: 0.5113\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.44213 to 0.51127, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 873s 2s/step - loss: 2.4001 - acc: 0.4405 - val_loss: 1.9796 - val_acc: 0.5258\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.51127 to 0.52579, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.3112 - acc: 0.4571 - val_loss: 1.9411 - val_acc: 0.5360\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.52579 to 0.53604, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 2.2759 - acc: 0.4652 - val_loss: 1.8953 - val_acc: 0.5404\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.53604 to 0.54041, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.2850 - acc: 0.4635 - val_loss: 1.9868 - val_acc: 0.5278\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.54041\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.3190 - acc: 0.4581 - val_loss: 2.0156 - val_acc: 0.5261\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.54041\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 2.3290 - acc: 0.4546 - val_loss: 2.0561 - val_acc: 0.5120\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.54041\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.3443 - acc: 0.4503 - val_loss: 1.9926 - val_acc: 0.5159\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.54041\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.3656 - acc: 0.4473 - val_loss: 2.0951 - val_acc: 0.5041\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.54041\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.3798 - acc: 0.4434 - val_loss: 2.1992 - val_acc: 0.4938\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.54041\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 869s 2s/step - loss: 2.4120 - acc: 0.4362 - val_loss: 2.4567 - val_acc: 0.4388\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.54041\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.4254 - acc: 0.4320 - val_loss: 2.4250 - val_acc: 0.4428\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.54041\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 873s 2s/step - loss: 2.4453 - acc: 0.4302 - val_loss: 2.4120 - val_acc: 0.4385\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.54041\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 873s 2s/step - loss: 2.4221 - acc: 0.4332 - val_loss: 2.2659 - val_acc: 0.4728\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.54041\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 2.3677 - acc: 0.4456 - val_loss: 2.3568 - val_acc: 0.4590\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.54041\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 874s 2s/step - loss: 2.3227 - acc: 0.4562 - val_loss: 2.0868 - val_acc: 0.5055\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.54041\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.2504 - acc: 0.4681 - val_loss: 2.0765 - val_acc: 0.5044\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.54041\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.1946 - acc: 0.4818 - val_loss: 2.1761 - val_acc: 0.5025\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.54041\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 870s 2s/step - loss: 2.1445 - acc: 0.4929 - val_loss: 2.0257 - val_acc: 0.5341\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.54041\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.0865 - acc: 0.5052 - val_loss: 1.9388 - val_acc: 0.5437\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.54041 to 0.54365, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 2.0215 - acc: 0.5173 - val_loss: 1.8905 - val_acc: 0.5614\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.54365 to 0.56142, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 1.9711 - acc: 0.5297 - val_loss: 1.8800 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.56142 to 0.56254, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 869s 2s/step - loss: 1.9282 - acc: 0.5419 - val_loss: 1.8452 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.56254 to 0.56772, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 1.9070 - acc: 0.5459 - val_loss: 1.9112 - val_acc: 0.5628\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.56772\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 872s 2s/step - loss: 1.9093 - acc: 0.5441 - val_loss: 1.8300 - val_acc: 0.5823\n",
            "\n",
            "Epoch 00042: val_acc improved from 0.56772 to 0.58234, saving model to /content/gdrive/My Drive/model_64_learn.h5\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 871s 2s/step - loss: 1.9253 - acc: 0.5397 - val_loss: 1.9120 - val_acc: 0.5617\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.58234\n",
            "Epoch 44/50\n",
            "424/500 [========================>.....] - ETA: 2:08 - loss: 1.9298 - acc: 0.5407"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}