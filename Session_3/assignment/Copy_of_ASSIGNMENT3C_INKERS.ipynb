{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ASSIGNMENT3C-INKERS.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edSiHPi2N5l",
        "colab_type": "code",
        "outputId": "5e474df1-ff26-43f1-93e0-8007b69d89fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 10\n",
        "num_filter = 20"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv8Z5QFI2Ujq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"/content/gdrive/My Drive/yolo/weights-improvement.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                            monitor='val_acc',\n",
        "                            verbose=1,\n",
        "                            save_best_only=True,\n",
        "                            mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hkvQwOH2cCO",
        "colab_type": "code",
        "outputId": "ec50b178-dccf-4902-cd60-cb82a14b8db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "print(img_height,x_test.shape[0],y_test.shape[0])\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32 10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztoDypc63gEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNnGm8Tv2fR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# Layer 1\n",
        "layer1 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='block_1_1', use_bias=False,)(input)\n",
        "layer1 = BatchNormalization(name='norm_0')(layer1)\n",
        "layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "\n",
        "\n",
        "layer2 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='block_1_2', use_bias=False)(layer1)\n",
        "layer2 = BatchNormalization(name='norm_1')(layer2)\n",
        "layer2 = LeakyReLU(alpha=0.1)(layer2)\n",
        "\n",
        "\n",
        "layer3 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='block_1_3', use_bias=False)(layer2)\n",
        "layer3 = BatchNormalization(name='norm_2')(layer3)\n",
        "layer3 = LeakyReLU(alpha=0.1)(layer3)\n",
        "\n",
        "\n",
        "layer4 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='block_1_4', use_bias=False)(layer3)\n",
        "layer4 = BatchNormalization(name='norm_3')(layer4)\n",
        "layer4 = LeakyReLU(alpha=0.1)(layer4)\n",
        "\n",
        "\n",
        "layer5 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='block_1_5', use_bias=False)(layer4)\n",
        "layer5 = BatchNormalization(name='norm_4')(layer5)\n",
        "layer5 = LeakyReLU(alpha=0.1)(layer5)\n",
        "\n",
        "layer5 = MaxPooling2D(pool_size=(2, 2))(layer5)\n",
        "skip_connection=layer5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#layer 2\n",
        "layer6 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='block_2_1', use_bias=False)(layer5)\n",
        "layer6 = BatchNormalization(name='norm_5')(layer6)\n",
        "layer6 = LeakyReLU(alpha=0.1)(layer6)\n",
        "\n",
        "\n",
        "\n",
        "layer7 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='block_2_2', use_bias=False)(layer6)\n",
        "layer7 = BatchNormalization(name='layer7')(layer7)\n",
        "layer7 = LeakyReLU(alpha=0.1)(layer7)\n",
        "\n",
        "\n",
        "layer8 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='block_2_3', use_bias=False)(layer7)\n",
        "layer8 = BatchNormalization(name='layer8')(layer8)\n",
        "\n",
        "layer8 = LeakyReLU(alpha=0.1)(layer8)\n",
        "\n",
        "layer9 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='block_2_4', use_bias=False)(layer8)\n",
        "layer9 = BatchNormalization(name='layer9')(layer9)\n",
        "layer9 = LeakyReLU(alpha=0.1)(layer9)\n",
        "\n",
        "\n",
        "layer10 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='block_2_5', use_bias=False)(layer9)\n",
        "layer10 = BatchNormalization(name='layer10')(layer10)\n",
        "layer10 = LeakyReLU(alpha=0.1)(layer10)\n",
        "\n",
        "\n",
        "layer10 = MaxPooling2D(pool_size=(2, 2))(layer10)\n",
        "\n",
        "#layer 3\n",
        "layer11 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='block_3_1', use_bias=False)(layer10)\n",
        "layer11 = BatchNormalization(name='layer11')(layer11)\n",
        "layer11 = LeakyReLU(alpha=0.1)(layer11)\n",
        "\n",
        "\n",
        "layer12 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='block_3_2', use_bias=False)(layer11)\n",
        "layer12 = BatchNormalization(name='layer12')(layer12)\n",
        "layer12 = LeakyReLU(alpha=0.1)(layer12)\n",
        "\n",
        "\n",
        "layer13 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='block_3_3', use_bias=False)(layer12)\n",
        "layer13 = BatchNormalization(name='layer13')(layer13)\n",
        "layer13 = LeakyReLU(alpha=0.1)(layer13)\n",
        "\n",
        "\n",
        "layer14 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='block_3_4', use_bias=False)(layer13)\n",
        "layer14 = BatchNormalization(name='layer14')(layer14)\n",
        "layer14 = LeakyReLU(alpha=0.1)(layer14)\n",
        "\n",
        "layer15 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='block_3_5', use_bias=False)(layer14)\n",
        "layer15 = BatchNormalization(name='layer15')(layer15)\n",
        "layer15 = LeakyReLU(alpha=0.1)(layer15)\n",
        "\n",
        "\n",
        "\n",
        "layer15 = MaxPooling2D(pool_size=(2, 2))(layer15)\n",
        "\n",
        "#layer 4\n",
        "layer16 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='block_4_1', use_bias=False)(layer15)\n",
        "layer16 = BatchNormalization(name='layer16')(layer16)\n",
        "layer16 = LeakyReLU(alpha=0.1)(layer16)\n",
        "\n",
        "\n",
        "layer17 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='block_4_2', use_bias=False)(layer16)\n",
        "layer17 = BatchNormalization(name='layer17')(layer17)\n",
        "layer17 = LeakyReLU(alpha=0.1)(layer17)\n",
        "\n",
        "layer18 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='block_4_3', use_bias=False)(layer17)\n",
        "layer18 = BatchNormalization(name='layer18')(layer18)\n",
        "\n",
        "layer18 = LeakyReLU(alpha=0.1)(layer18)\n",
        "layer19 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='block_4_4', use_bias=False)(layer18)\n",
        "layer19 = BatchNormalization(name='layer19')(layer19)\n",
        "layer19 = LeakyReLU(alpha=0.1)(layer19)\n",
        "\n",
        "layer20 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='block_4_5', use_bias=False)(layer19)\n",
        "layer20 = BatchNormalization(name='layer20')(layer20)\n",
        "layer20 = LeakyReLU(alpha=0.1)(layer20)\n",
        "\n",
        "\n",
        "\n",
        "layer20 = MaxPooling2D(pool_size=(2, 2))(layer20)\n",
        "skip_connection = Conv2D(32, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "\n",
        "\n",
        "layer21 = concatenate([skip_connection, layer20])\n",
        "\n",
        "\n",
        "\n",
        "# Layer 23\n",
        "layer22 = Flatten()(layer21)\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(layer22)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jeh0VAxy26NV",
        "colab_type": "code",
        "outputId": "4cbab573-7ea2-45db-c8aa-ab79981a74c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2570
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block_1_1 (Conv2D)              (None, 32, 32, 32)   864         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_0 (BatchNormalization)     (None, 32, 32, 32)   128         block_1_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_103 (LeakyReLU)     (None, 32, 32, 32)   0           norm_0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_2 (Conv2D)              (None, 32, 32, 64)   18432       leaky_re_lu_103[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 32, 32, 64)   256         block_1_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_104 (LeakyReLU)     (None, 32, 32, 64)   0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_3 (Conv2D)              (None, 32, 32, 128)  73728       leaky_re_lu_104[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_2 (BatchNormalization)     (None, 32, 32, 128)  512         block_1_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_105 (LeakyReLU)     (None, 32, 32, 128)  0           norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_4 (Conv2D)              (None, 32, 32, 256)  294912      leaky_re_lu_105[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 32, 32, 256)  1024        block_1_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_106 (LeakyReLU)     (None, 32, 32, 256)  0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_5 (Conv2D)              (None, 32, 32, 512)  1179648     leaky_re_lu_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalization)     (None, 32, 32, 512)  2048        block_1_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_107 (LeakyReLU)     (None, 32, 32, 512)  0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling2D) (None, 16, 16, 512)  0           leaky_re_lu_107[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_1 (Conv2D)              (None, 16, 16, 32)   147456      max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "norm_5 (BatchNormalization)     (None, 16, 16, 32)   128         block_2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_108 (LeakyReLU)     (None, 16, 16, 32)   0           norm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_2 (Conv2D)              (None, 16, 16, 64)   18432       leaky_re_lu_108[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer7 (BatchNormalization)     (None, 16, 16, 64)   256         block_2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_109 (LeakyReLU)     (None, 16, 16, 64)   0           layer7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_3 (Conv2D)              (None, 16, 16, 128)  73728       leaky_re_lu_109[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer8 (BatchNormalization)     (None, 16, 16, 128)  512         block_2_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_110 (LeakyReLU)     (None, 16, 16, 128)  0           layer8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_4 (Conv2D)              (None, 16, 16, 256)  294912      leaky_re_lu_110[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer9 (BatchNormalization)     (None, 16, 16, 256)  1024        block_2_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_111 (LeakyReLU)     (None, 16, 16, 256)  0           layer9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_5 (Conv2D)              (None, 16, 16, 512)  1179648     leaky_re_lu_111[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer10 (BatchNormalization)    (None, 16, 16, 512)  2048        block_2_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_112 (LeakyReLU)     (None, 16, 16, 512)  0           layer10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling2D) (None, 8, 8, 512)    0           leaky_re_lu_112[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_3_1 (Conv2D)              (None, 8, 8, 32)     147456      max_pooling2d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer11 (BatchNormalization)    (None, 8, 8, 32)     128         block_3_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_113 (LeakyReLU)     (None, 8, 8, 32)     0           layer11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_3_2 (Conv2D)              (None, 8, 8, 64)     18432       leaky_re_lu_113[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer12 (BatchNormalization)    (None, 8, 8, 64)     256         block_3_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_114 (LeakyReLU)     (None, 8, 8, 64)     0           layer12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_3_3 (Conv2D)              (None, 8, 8, 128)    73728       leaky_re_lu_114[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer13 (BatchNormalization)    (None, 8, 8, 128)    512         block_3_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_115 (LeakyReLU)     (None, 8, 8, 128)    0           layer13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_3_4 (Conv2D)              (None, 8, 8, 256)    294912      leaky_re_lu_115[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer14 (BatchNormalization)    (None, 8, 8, 256)    1024        block_3_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_116 (LeakyReLU)     (None, 8, 8, 256)    0           layer14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_3_5 (Conv2D)              (None, 8, 8, 512)    1179648     leaky_re_lu_116[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer15 (BatchNormalization)    (None, 8, 8, 512)    2048        block_3_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_117 (LeakyReLU)     (None, 8, 8, 512)    0           layer15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling2D) (None, 4, 4, 512)    0           leaky_re_lu_117[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_1 (Conv2D)              (None, 4, 4, 32)     147456      max_pooling2d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer16 (BatchNormalization)    (None, 4, 4, 32)     128         block_4_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_118 (LeakyReLU)     (None, 4, 4, 32)     0           layer16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_4_2 (Conv2D)              (None, 4, 4, 64)     18432       leaky_re_lu_118[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer17 (BatchNormalization)    (None, 4, 4, 64)     256         block_4_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_119 (LeakyReLU)     (None, 4, 4, 64)     0           layer17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_4_3 (Conv2D)              (None, 4, 4, 128)    73728       leaky_re_lu_119[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer18 (BatchNormalization)    (None, 4, 4, 128)    512         block_4_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_120 (LeakyReLU)     (None, 4, 4, 128)    0           layer18[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block_4_4 (Conv2D)              (None, 4, 4, 256)    294912      leaky_re_lu_120[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer19 (BatchNormalization)    (None, 4, 4, 256)    1024        block_4_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_121 (LeakyReLU)     (None, 4, 4, 256)    0           layer19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_21 (Conv2D)                (None, 16, 16, 32)   16384       max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_4_5 (Conv2D)              (None, 4, 4, 512)    1179648     leaky_re_lu_121[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_21 (BatchNormalization)    (None, 16, 16, 32)   128         conv_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer20 (BatchNormalization)    (None, 4, 4, 512)    2048        block_4_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_123 (LeakyReLU)     (None, 16, 16, 32)   0           norm_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_122 (LeakyReLU)     (None, 4, 4, 512)    0           layer20[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 2, 2, 2048)   0           leaky_re_lu_123[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling2D) (None, 2, 2, 512)    0           leaky_re_lu_122[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 2, 2, 2560)   0           lambda_7[0][0]                   \n",
            "                                                                 max_pooling2d_24[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 10240)        0           concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           102410      flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,844,906\n",
            "Trainable params: 6,836,906\n",
            "Non-trainable params: 8,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apCwOjvZ4Kts",
        "colab_type": "code",
        "outputId": "ec11a3f7-009f-4c79-ff8d-106a89ccdfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1730
        }
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 294s 6ms/step - loss: 1.5702 - acc: 0.4850 - val_loss: 1.3527 - val_acc: 0.5287\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 281s 6ms/step - loss: 1.1672 - acc: 0.6208 - val_loss: 1.1772 - val_acc: 0.6202\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 281s 6ms/step - loss: 0.8965 - acc: 0.7003 - val_loss: 1.0127 - val_acc: 0.6582\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 281s 6ms/step - loss: 0.7321 - acc: 0.7452 - val_loss: 0.8842 - val_acc: 0.7002\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 278s 6ms/step - loss: 0.6387 - acc: 0.7773 - val_loss: 1.0549 - val_acc: 0.6669\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 0.5634 - acc: 0.8047 - val_loss: 0.9997 - val_acc: 0.6892\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 296s 6ms/step - loss: 0.4952 - acc: 0.8273 - val_loss: 0.8390 - val_acc: 0.7344\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 280s 6ms/step - loss: 0.4181 - acc: 0.8547 - val_loss: 1.2794 - val_acc: 0.6358\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 308s 6ms/step - loss: 0.3589 - acc: 0.8746 - val_loss: 1.1140 - val_acc: 0.6954\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 296s 6ms/step - loss: 0.3002 - acc: 0.8958 - val_loss: 0.9352 - val_acc: 0.7314\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 0.2509 - acc: 0.9121 - val_loss: 1.0483 - val_acc: 0.7097\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 0.1974 - acc: 0.9332 - val_loss: 1.0432 - val_acc: 0.7270\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 297s 6ms/step - loss: 0.1602 - acc: 0.9442 - val_loss: 1.1610 - val_acc: 0.7168\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 302s 6ms/step - loss: 0.1290 - acc: 0.9566 - val_loss: 1.2697 - val_acc: 0.7167\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 300s 6ms/step - loss: 0.1101 - acc: 0.9618 - val_loss: 2.0576 - val_acc: 0.6325\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 228s 5ms/step - loss: 0.0873 - acc: 0.9711 - val_loss: 1.2350 - val_acc: 0.7296\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0861 - acc: 0.9703 - val_loss: 1.3083 - val_acc: 0.7313\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 300s 6ms/step - loss: 0.0890 - acc: 0.9685 - val_loss: 1.5324 - val_acc: 0.7105\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 301s 6ms/step - loss: 0.0590 - acc: 0.9800 - val_loss: 1.3775 - val_acc: 0.7383\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 265s 5ms/step - loss: 0.0714 - acc: 0.9752 - val_loss: 1.5251 - val_acc: 0.7278\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0800 - acc: 0.9713 - val_loss: 1.4919 - val_acc: 0.7332\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 228s 5ms/step - loss: 0.0481 - acc: 0.9836 - val_loss: 1.4720 - val_acc: 0.7236\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 228s 5ms/step - loss: 0.0373 - acc: 0.9877 - val_loss: 1.4248 - val_acc: 0.7433\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 283s 6ms/step - loss: 0.0504 - acc: 0.9825 - val_loss: 1.5819 - val_acc: 0.7268\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 307s 6ms/step - loss: 0.0541 - acc: 0.9806 - val_loss: 1.6527 - val_acc: 0.7236\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 292s 6ms/step - loss: 0.0681 - acc: 0.9757 - val_loss: 1.6740 - val_acc: 0.7231\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0341 - acc: 0.9886 - val_loss: 1.5281 - val_acc: 0.7425\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0304 - acc: 0.9895 - val_loss: 1.5655 - val_acc: 0.7363\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0436 - acc: 0.9844 - val_loss: 1.8605 - val_acc: 0.7065\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0473 - acc: 0.9836 - val_loss: 1.7112 - val_acc: 0.7261\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0451 - acc: 0.9840 - val_loss: 1.7045 - val_acc: 0.7328\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0357 - acc: 0.9874 - val_loss: 1.5565 - val_acc: 0.7419\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0224 - acc: 0.9926 - val_loss: 1.6751 - val_acc: 0.7352\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0266 - acc: 0.9907 - val_loss: 1.7783 - val_acc: 0.7347\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0364 - acc: 0.9871 - val_loss: 1.8685 - val_acc: 0.7240\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0349 - acc: 0.9875 - val_loss: 1.9116 - val_acc: 0.7230\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0406 - acc: 0.9857 - val_loss: 1.8964 - val_acc: 0.7256\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0285 - acc: 0.9901 - val_loss: 1.7611 - val_acc: 0.7357\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0254 - acc: 0.9910 - val_loss: 1.8579 - val_acc: 0.7270\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0213 - acc: 0.9927 - val_loss: 1.7475 - val_acc: 0.7452\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0283 - acc: 0.9903 - val_loss: 1.9320 - val_acc: 0.7308\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0396 - acc: 0.9858 - val_loss: 1.9646 - val_acc: 0.7125\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0268 - acc: 0.9905 - val_loss: 2.0118 - val_acc: 0.7245\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0265 - acc: 0.9909 - val_loss: 1.7721 - val_acc: 0.7343\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0174 - acc: 0.9943 - val_loss: 2.0951 - val_acc: 0.7195\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0190 - acc: 0.9934 - val_loss: 1.8940 - val_acc: 0.7362\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0203 - acc: 0.9931 - val_loss: 1.8930 - val_acc: 0.7390\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0343 - acc: 0.9881 - val_loss: 1.9095 - val_acc: 0.7215\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0271 - acc: 0.9911 - val_loss: 1.9793 - val_acc: 0.7337\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 229s 5ms/step - loss: 0.0213 - acc: 0.9927 - val_loss: 1.8125 - val_acc: 0.7489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2f6f9e3b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JteK9bPABJtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls -lart \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTfZIGrf4Uyd",
        "colab_type": "code",
        "outputId": "a59d5232-7752-46ed-db42-9362b538f3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"Yolo_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 17s 2ms/step\n",
            "Test loss: 1.8124542695045471\n",
            "Test accuracy: 0.7489\n",
            "Saved the model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyb9TlVGsZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"Yolo_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")\n",
        "from google.colab import files\n",
        "\n",
        "files.download('Yolo_Basic_model2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9A3pesKbUJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}